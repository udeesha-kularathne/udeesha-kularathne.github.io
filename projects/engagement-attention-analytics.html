<!-- projects/engagement-attention-analytics.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real-Time Engagement & Attention Analytics | Udeesha Kularathne</title>
    <meta name="description" content="A real-time computer vision system that fuses emotion recognition, head pose, and eye gaze into an engagement score with a dashboard-ready backend.">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="../assets/css/projects.css">
</head>

<body>
<div class="cursor"></div>
<div class="cursor-follower"></div>
<div class="noise"></div>

<!-- Navigation -->
<nav class="nav scrolled" id="nav">
    <div class="nav-container">
        <a href="../index.html" class="nav-logo">
            <span class="logo-text">Udeesha Kularathne</span>
        </a>
        <div class="nav-links">
            <a href="../index.html#about" class="nav-link" data-text="About">About</a>
            <a href="../index.html#experience" class="nav-link" data-text="Experience">Experience</a>
            <a href="../projects.html" class="nav-link active" data-text="Projects">Projects</a>
            <a href="../blog.html" class="nav-link" data-text="Blog">Blog</a>
            <a href="../index.html#contact" class="nav-link" data-text="Contact">Contact</a>
        </div>
        <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
            <span></span><span></span><span></span>
        </button>
    </div>
</nav>

<!-- Mobile Navigation -->
<div class="mobile-nav" id="mobileNav">
    <div class="mobile-nav-content">
        <a href="../index.html#about" class="mobile-nav-link">About</a>
        <a href="../index.html#experience" class="mobile-nav-link">Experience</a>
        <a href="../projects.html" class="mobile-nav-link">Projects</a>
        <a href="../blog.html" class="mobile-nav-link">Blog</a>
        <a href="../index.html#contact" class="mobile-nav-link">Contact</a>
    </div>
</div>

<!-- Project Hero -->
<section class="project-hero">
    <div class="container">
        <div class="project-hero-content">
            <div class="project-breadcrumb">
                <a href="../index.html">Home</a>
                <span>/</span>
                <a href="../projects.html">Projects</a>
                <span>/</span>
                <span>Engagement Analytics</span>
            </div>

            <h1 class="project-hero-title">Real-Time Engagement & Attention Analytics</h1>

            <p class="project-hero-description">
                A real-time computer vision system that fuses emotion recognition, head pose, and eye gaze
                into an interpretable engagement score. Designed for analytics dashboards and time-series reporting,
                with a backend pipeline built for structured storage, aggregation, and future product extension.
            </p>

            <div class="project-meta">
                <div class="meta-item">
                    <span class="meta-label">Role</span>
                    <span class="meta-value">Full-Stack Developer & AI Engineer</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Timeline</span>
                    <span class="meta-value">2024 - 2025</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Category</span>
                    <span class="meta-value">AI/ML, Computer Vision, Web Application</span>
                </div>
            </div>
        </div>
    </div>

    <div class="hero-bg">
        <div class="gradient-sphere gradient-sphere-1"></div>
        <div class="gradient-sphere gradient-sphere-2"></div>
        <div class="grid-lines"></div>
    </div>
</section>

<!-- Project Image -->
<div class="container">
    <div class="blog-cover">
        <div
                class="project-cover-image"
                aria-label="Engagement analytics cover illustration"
                style="background-image: url('../assets/images/projects/engagement.png');">
        </div>
    </div>
</div>

<!-- Overview -->
<section class="project-content-section">
    <div class="container">
        <h2>Project Overview</h2>

        <p>
            This project was built to convert raw webcam/video signals into measurable, dashboard-ready engagement insights.
            Instead of relying on a single model or a single signal, the system combines multiple perception layers:
            emotion recognition, attention cues (eye gaze), and posture/orientation cues (head pose). The outcome is an
            interpretable engagement score that can be monitored live and analyzed historically.
        </p>

        <p>
            The ML pipeline follows a hybrid approach: a “basic emotion” model handles common facial emotion categories,
            while a “complex emotion” model focuses on engagement-related states (such as boredom, confusion, and frustration).
            These predictions are fused with gaze and head-pose signals to compute a final engagement score and a readable
            engagement status (engaged / partially engaged / not engaged).
        </p>

        <p>
            On the engineering side, the system is designed as a product-ready workflow. Live inference produces structured
            JSON outputs, a backend API stores time-series logs and exposes reporting endpoints, and the frontend can consume
            both real-time and aggregated summaries for charts, dashboards, and operational reporting.
        </p>

        <p style="font-size: 0.9rem; color: var(--text-muted);">
            Note: To protect commercial usability and privacy, identity-specific tracking, deployment thresholds,
            dataset sourcing details, and operational monitoring rules are intentionally abstracted.
        </p>
    </div>
</section>

<!-- Features -->
<section class="project-content-section">
    <div class="container">
        <h2>Key Features</h2>

        <div class="features-grid">

            <div class="feature-card">
                <div class="feature-icon">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M12 2l8 4v6c0 5-3.5 9-8 10-4.5-1-8-5-8-10V6l8-4z"/>
                    </svg>
                </div>
                <h3>Dual-Model Emotion Recognition</h3>
                <p>
                    Uses two specialized emotion models: one for standard facial emotions and another for engagement-oriented
                    states. This separation improves signal clarity and reduces confusion between “mood” and “engagement”.
                </p>
            </div>

            <div class="feature-card">
                <div class="feature-icon">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M3 3v18h18"/>
                        <path d="M7 15l3-3 3 2 5-6"/>
                    </svg>
                </div>
                <h3>Attention Signals via Eye-Gaze</h3>
                <p>
                    Estimates horizontal gaze direction using facial landmarks (and iris landmarks when available),
                    producing a calibrated gaze signal that supports distraction detection and attention scoring.
                </p>
            </div>

            <div class="feature-card">
                <div class="feature-icon">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="10"/>
                        <path d="M12 6v6l4 2"/>
                    </svg>
                </div>
                <h3>Head Pose Estimation</h3>
                <p>
                    Computes pitch, yaw, and roll from facial landmarks using a geometric pose solver, adding a strong
                    posture-based cue to distinguish “looking away” from genuine emotional states.
                </p>
            </div>

            <div class="feature-card">
                <div class="feature-icon">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M4 19h16"/>
                        <path d="M4 15l4-4 4 3 6-7"/>
                    </svg>
                </div>
                <h3>Engagement Score Fusion</h3>
                <p>
                    Produces a final engagement score by combining emotion negativity signals with distraction components.
                    Outputs are designed to be interpretable and stable for dashboards, not just model-centric probabilities.
                </p>
            </div>

            <div class="feature-card">
                <div class="feature-icon">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M12 2l3 6 7 .5-5 4.2 1.6 6.8L12 16l-6.6 3.5L7 12.7 2 8.5 9 8l3-6z"/>
                    </svg>
                </div>
                <h3>Explainability with Grad-CAM</h3>
                <p>
                    Supports heatmap-based visual explanations to verify what the model focuses on during prediction,
                    helping validate behavior, reduce debugging time, and improve trust in the pipeline.
                </p>
            </div>

            <div class="feature-card">
                <div class="feature-icon">
                    <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <rect x="3" y="3" width="18" height="18" rx="2"/>
                        <path d="M7 7h10M7 12h10M7 17h10"/>
                    </svg>
                </div>
                <h3>Time-Series Logging & Reporting API</h3>
                <p>
                    Stores structured ML outputs as time-series logs and exposes endpoints for engagement history,
                    daily aggregation, and report-ready summaries designed for analytics dashboards.
                </p>
            </div>

        </div>
    </div>
</section>

<!-- Tech Stack -->
<section class="tech-stack-section">
    <div class="container">
        <h2>Technology & Engineering Stack</h2>

        <div class="tech-grid">
            <span class="tech-item">Python</span>
            <span class="tech-item">TensorFlow / Keras</span>
            <span class="tech-item">Computer Vision</span>
            <span class="tech-item">Emotion Recognition</span>
            <span class="tech-item">Grad-CAM Explainability</span>

            <span class="tech-item">MediaPipe Face Mesh</span>
            <span class="tech-item">Eye-Gaze Estimation</span>
            <span class="tech-item">Head Pose (PnP Solver)</span>
            <span class="tech-item">Real-Time Inference</span>

            <span class="tech-item">Node.js</span>
            <span class="tech-item">Express.js</span>
            <span class="tech-item">REST APIs</span>

            <span class="tech-item">Supabase</span>
            <span class="tech-item">PostgreSQL</span>
            <span class="tech-item">JSONB Time-Series Storage</span>

            <span class="tech-item">Modular Backend Design</span>
            <span class="tech-item">Separation of Concerns</span>
            <span class="tech-item">Maintainable API Architecture</span>
        </div>
    </div>
</section>

<!-- Results -->
<section class="project-content-section">
    <div class="container">
        <h2>Results & Engineering Impact</h2>

        <p>
            The key outcome of this project is a practical, real-time engagement measurement workflow that behaves like a product,
            not just a model demo. The system produces stable, interpretable engagement outputs by combining multiple weak signals
            into a single score: emotion probabilities, attention direction (eye gaze), and posture/orientation (head pose). This
            multi-signal fusion improves robustness compared to relying on a single emotion classifier that can be brittle under
            lighting changes, head movement, or noisy facial expressions.
        </p>

        <p>
            On the ML side, the dual-model strategy separates “general emotions” from “engagement-related states”, improving semantic
            clarity. The pipeline also supports explainability through Grad-CAM heatmaps, which is crucial for validating whether the
            model is focusing on meaningful facial regions rather than background artifacts. For real-time usability, the inference
            flow is designed to be efficient: lightweight preprocessing, model inference, signal fusion, and structured output generation.
        </p>

        <p>
            On the backend, the project applies strong data engineering decisions that make analytics scalable. Instead of storing only
            single-point predictions, the backend stores time-series ML outputs as structured JSON records per day, enabling both granular
            replay and aggregated reporting. Reporting endpoints compute daily summaries (engagement averages, distraction trends, and emotion
            distributions) so the frontend can render dashboards efficiently without overloading the client with raw data processing.
        </p>

        <p>
            On the frontend/product layer, the architecture is designed to support real dashboard use cases: real-time monitoring, historical
            trend exploration, and report generation. The output schema is consistent and intentionally “dashboard-first”, meaning every prediction
            is stored in a format that is easy to chart, filter, and summarize. This makes the solution directly usable for operational review and
            long-term tracking rather than being limited to real-time overlays.
        </p>

        <p>
            The project also demonstrates software engineering principles that matter in production environments:
            separation of concerns (ML inference vs API storage vs reporting), modular controllers and routes, a clean service layer for database
            access, and predictable interfaces between components. The system is intentionally designed so each layer can evolve independently:
            models can be replaced without rewriting the backend schema, reporting logic can expand without changing inference, and the UI can add
            new dashboards without touching ML internals.
        </p>

        <p>
            Privacy and commercial sensitivity are treated as first-class constraints. The design focuses on engagement signals and analytics rather
            than identity. Operational thresholds, dataset sourcing specifics, and any identity-related features are deliberately abstracted, keeping the
            portfolio description meaningful while protecting deployment details and commercial applicability.
        </p>
    </div>
</section>

<!-- Navigation -->
<section class="container">
    <div class="project-nav">
        <a href="hybrid-malware-detection.html" class="project-nav-link">
            <span class="project-nav-label">Previous Project</span>
            <span class="project-nav-title">Hybrid Malware Detection System</span>
        </a>
        <a href="aflatoxin-detection.html" class="project-nav-link">
            <span class="project-nav-label">Next Project</span>
            <span class="project-nav-title">Aflatoxin Detection & Yield Prediction System</span>
        </a>
    </div>
</section>

<!-- Footer -->
<footer class="footer">
    <div class="container">
        <div class="footer-content">
            <div class="footer-brand">
                <span class="footer-logo">Udeesha Kularathne</span>
                <p>Building the future with code and AI.</p>
            </div>
            <div class="footer-links">
                <a href="../index.html#about">About</a>
                <a href="../index.html#experience">Experience</a>
                <a href="../projects.html">Projects</a>
                <a href="../blog.html">Blog</a>
                <a href="../index.html#contact">Contact</a>
            </div>
            <div class="footer-social">
                <a href="https://www.linkedin.com/in/udeeshakularathne" target="_blank" rel="noopener" aria-label="LinkedIn">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"/>
                        <rect x="2" y="9" width="4" height="12"/>
                        <circle cx="4" cy="4" r="2"/>
                    </svg>
                </a>
                <a href="mailto:umahinsab@gmail.com" aria-label="Email">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
                        <polyline points="22,6 12,13 2,6"/>
                    </svg>
                </a>
            </div>
        </div>
        <div class="footer-bottom">
            <p>&copy; 2026 Udeesha Kularathne. All rights reserved.</p>
        </div>
    </div>
</footer>

<script src="../assets/js/main.js"></script>
</body>
</html>
